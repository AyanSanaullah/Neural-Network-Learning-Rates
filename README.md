# Neural-Network-Learning-Rate-Analysis

This project explores the effect of different learning rates on the training and performance of a simple neural network. The network is implemented from scratch using Python and key libraries such as NumPy and Matplotlib. The analysis demonstrates how learning rates influence convergence speed, stability, and overall training performance.

This project trains a single-layer neural network on a small dataset for binary classification, visualizing decision boundaries and learning curves for multiple learning rates.

## Features

- **Custom Neural Network:** A single-layer neural network with sigmoid activation, implemented from scratch.
- **Learning Rate Comparison:** Evaluates learning rates (1, 0.5, 0.1, 0.01) to observe their effects on training dynamics.
- **Binary Classification:** Classifies a dataset of 2D points with binary labels.
- **Visualization:**
  - Plots decision boundaries for each learning rate.
  - Displays training cost curves over epochs.

## Usage

- Open the Jupyter Notebook.

  Run the cells step by step to:
  - Define and visualize the dataset.
  - Train the neural network with different learning rates.
  - Visualize decision boundaries.
  - Plot training cost curves.

## Future Work

- Extend the model to handle multi-class classification tasks.
- Implement additional activation functions such as ReLU and Tanh.
- Use a larger and more complex dataset for deeper insights.
- Compare results with neural networks implemented in TensorFlow or PyTorch.

## Project Structure

- The main notebook contains all code, training results, and visualizations.

## License

This project is licensed under the Apache License 2.0.
